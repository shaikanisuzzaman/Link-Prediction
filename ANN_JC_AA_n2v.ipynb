{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "import talos\n",
    "from talos.utils import lr_normalizer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16087\n"
     ]
    }
   ],
   "source": [
    "nodes = dict()\n",
    "\n",
    "with open(\"train-mod.txt\") as file :\n",
    "    end = file.seek(0, 2)\n",
    "    file.seek(0)\n",
    "    while file.tell() != end:\n",
    "        line = file.readline().split()\n",
    "        edges = list(itertools.combinations(line,2))\n",
    "        for i in edges:\n",
    "            if nodes.get(i) == None:\n",
    "                node1 = i[0]\n",
    "                node2 = i[1]\n",
    "                if nodes.get((node2,node1)) == None:\n",
    "                    nodes[i] = 1\n",
    "                else:\n",
    "                    nodes[(node2,node1)] += 1\n",
    "            else:\n",
    "                nodes[i] +=1\n",
    "\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weighted_graph.csv\", \"w\", newline=\"\") as a_file:\n",
    "\n",
    "    writer = csv.writer(a_file)\n",
    "    for key, value in nodes.items():\n",
    "        writer.writerow([key[0], key[1], value])\n",
    "\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_weighted_edgelist('weighted_graph.csv', delimiter=',', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 3816\n",
      "Number of edges: 16087\n",
      "Average degree:   8.4313\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_pos = list(nodes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edges_pos_all.csv\",\"w\",newline=\"\") as csvfile:\n",
    "    writer=csv.writer(csvfile)\n",
    "    writer.writerow([\"Source\",\"Target\", \"Label\"])\n",
    "    for edge in edges_pos:\n",
    "        writer.writerow([edge[0], edge[1], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating negative edges (random sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "num_test_edges = 16087\n",
    "edges_neg = []\n",
    "while i < num_test_edges:\n",
    "    edge = random.sample(g.nodes(), 2)\n",
    "    try:\n",
    "        edge_exists = g.has_edge(edge[0],edge[1])\n",
    "        if edge_exists == False:\n",
    "            edges_neg.append([edge[0],edge[1]])\n",
    "            i = i+1\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edges_neg_16k.csv\",\"w\",newline=\"\") as csvfile:\n",
    "    writer=csv.writer(csvfile)\n",
    "    writer.writerow([\"Source\",\"Target\", \"Label\"])\n",
    "    for edge in edges_neg:\n",
    "        writer.writerow([edge[0], edge[1], 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_positive = pd.read_csv('edges_pos_all.csv').to_numpy()\n",
    "edges_negative = pd.read_csv('edges_neg_16k.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading total data df\n",
    "df_pos = pd.DataFrame(edges_positive, columns=['source_node', 'destination_node', 'label'])\n",
    "df_neg = pd.DataFrame(edges_negative, columns=['source_node', 'destination_node', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_n2v =  df_pos[['source_node', 'destination_node']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3816/3816 [00:02<00:00, 1650.16it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 30/30 [03:47<00:00,  7.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate walks\n",
    "node2vec = Node2Vec(g, dimensions=128, walk_length=80, num_walks=30)\n",
    "\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_pos, df_neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n2v = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(data['source_node'], data['destination_node'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(sample_list, test = False):\n",
    "    features = []\n",
    "    i = 0\n",
    "    for sample in sample_list:\n",
    "        #print(sample)\n",
    "        source = sample[0]\n",
    "        target = sample[1]\n",
    "        if test == False:\n",
    "            label = sample[2]\n",
    "        else:\n",
    "            label = -1\n",
    "        \n",
    "        feature = []\n",
    "        try:\n",
    "            i = i+1\n",
    "            #print(i)\n",
    "            \n",
    "            #p = nx.common_neighbors(g, source, target)\n",
    "            #feature.append(len(p))\n",
    "            \n",
    "            #p = nx.simrank_similarity(g, source, target)\n",
    "            #feature.append(p)\n",
    "            \n",
    "            #preds = nx.resource_allocation_index(g, [(source, target)])\n",
    "            #for u, v, p in preds:\n",
    "            #    feature.append(p)\n",
    "\n",
    "            preds = nx.jaccard_coefficient(g, [(source, target)])\n",
    "            for u, v, p in preds:\n",
    "                feature.append(p)\n",
    "\n",
    "            preds = nx.adamic_adar_index(g, [(source, target)])\n",
    "            for u, v, p in preds:\n",
    "                feature.append(p)\n",
    "\n",
    "            #preds = nx.preferential_attachment(g, [(source, target)])\n",
    "            #for u, v, p in preds:\n",
    "            #    feature.append(p)\n",
    "            \n",
    "            feature.append(label)  # append label\n",
    "            \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "        features.append(feature)\n",
    "    print(\"features: \"+str(len(features)))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 16087\n",
      "features: 16087\n"
     ]
    }
   ],
   "source": [
    "features_pos = generate_features(edges_positive)\n",
    "features_neg = generate_features(edges_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_pos + features_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_train_to_csv(features):\n",
    "    with open(\"train_16k_sim.csv\",\"w\",newline=\"\") as csvfile:\n",
    "        writer=csv.writer(csvfile)\n",
    "        writer.writerow([\"JC\",\"AA\",\"Label\"])\n",
    "        writer.writerows(features)\n",
    "        \n",
    "write_train_to_csv(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sim = pd.read_csv('train_16k_sim.csv')\n",
    "FEATURE_SIZE=2\n",
    "\n",
    "X_sim = dataset_sim.iloc[:,:FEATURE_SIZE].values\n",
    "y_sim = dataset_sim.iloc[:,FEATURE_SIZE].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n2v_1 = np.array(x_train_n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_sim_1 = sc.fit_transform(X_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = np.concatenate((X_sim_1,x_train_n2v_1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to make sure to input data and params into the function\n",
    "def sml_model(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation=params['activation'],\n",
    "                    kernel_initializer=params['kernel_initializer']))\n",
    "    \n",
    "    model.add(Dropout(params['dropout']))\n",
    "\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer']))\n",
    "    \n",
    "    model.compile(loss=params['losses'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=['acc', talos.utils.metrics.f1score])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks=[talos.utils.early_stopper(params['epochs'])])\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can go ahead and set the parameter space\n",
    "p = {'first_neuron':[128, 512],\n",
    "     'hidden_layers':[2, 3, 4],\n",
    "     'batch_size': [128],\n",
    "     'epochs': [100],\n",
    "     'dropout': (0, 0.2, 0.40, 10),\n",
    "     'kernel_initializer': ['uniform','normal'],\n",
    "     'optimizer': ['Adam', 'SGD'],\n",
    "     'losses': ['binary_crossentropy'],\n",
    "     'activation':['relu'],\n",
    "     'last_activation': ['sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [14:23<00:00, 35.96s/it]\n"
     ]
    }
   ],
   "source": [
    "scan_object = talos.Scan(x=all_feats,\n",
    "                         y=labels, \n",
    "                         params=p,\n",
    "                         model=sml_model,\n",
    "                         experiment_name='smlproj_2'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Scan object as input\n",
    "analyze_object = talos.Analyze(scan_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['binary_crossentropy', 'SGD', 0.0, 512, '04/13/21-112430',\n",
       "        'relu', 2, 128, 77, 75.20409274101257, '04/13/21-112545',\n",
       "        0.9549400806427002, 100, 0.9840306639671326, 'uniform',\n",
       "        'sigmoid', 0],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 128, '04/13/21-112027',\n",
       "        'relu', 3, 128, 77, 47.13001203536987, '04/13/21-112114',\n",
       "        0.9540058970451355, 100, 0.97808438539505, 'uniform', 'sigmoid',\n",
       "        1],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 512, '04/13/21-112720',\n",
       "        'relu', 3, 128, 73, 69.19339895248413, '04/13/21-112829',\n",
       "        0.9531311988830566, 100, 0.984320878982544, 'uniform', 'sigmoid',\n",
       "        2],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 128, '04/13/21-112326',\n",
       "        'relu', 4, 128, 68, 42.83750820159912, '04/13/21-112409',\n",
       "        0.9527631998062134, 100, 0.9783999919891357, 'normal', 'sigmoid',\n",
       "        3],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 128, '04/13/21-111836',\n",
       "        'relu', 2, 128, 74, 46.18578100204468, '04/13/21-111922',\n",
       "        0.9526097178459167, 100, 0.9771654009819031, 'uniform',\n",
       "        'sigmoid', 4],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 512, '04/13/21-113007',\n",
       "        'relu', 4, 128, 71, 68.36855173110962, '04/13/21-113116',\n",
       "        0.9523329734802246, 100, 0.9836141467094421, 'uniform',\n",
       "        'sigmoid', 5],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 128, '04/13/21-112227',\n",
       "        'relu', 4, 128, 74, 47.25611615180969, '04/13/21-112314',\n",
       "        0.952031672000885, 100, 0.9782227873802185, 'uniform', 'sigmoid',\n",
       "        6],\n",
       "       ['binary_crossentropy', 'Adam', 0.0, 128, '04/13/21-112214',\n",
       "        'relu', 4, 128, 19, 13.209186792373657, '04/13/21-112227',\n",
       "        0.9510003328323364, 100, 0.9783210158348083, 'uniform',\n",
       "        'sigmoid', 7],\n",
       "       ['binary_crossentropy', 'SGD', 0.0, 512, '04/13/21-113130',\n",
       "        'relu', 4, 128, 75, 75.71765184402466, '04/13/21-113246',\n",
       "        0.9506799578666687, 100, 0.9873055815696716, 'normal', 'sigmoid',\n",
       "        8],\n",
       "       ['binary_crossentropy', 'Adam', 0.0, 128, '04/13/21-112017',\n",
       "        'relu', 3, 128, 15, 10.128566026687622, '04/13/21-112027',\n",
       "        0.9505782127380371, 100, 0.9758960604667664, 'uniform',\n",
       "        'sigmoid', 9]], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best paramaters\n",
    "analyze_object.best_params('val_acc', ['acc', 'loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9759900218272529,\n",
       " 0.9754562617998742,\n",
       " 0.9779022720199191,\n",
       " 0.9784263959390863,\n",
       " 0.9788161993769471,\n",
       " 0.9717868338557993,\n",
       " 0.9743268628678773,\n",
       " 0.9750394944707741,\n",
       " 0.9746001881467544,\n",
       " 0.9780743565300286]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_object = talos.Evaluate(scan_object)\n",
    "evaluate_object.evaluate(x=all_feats,\n",
    "                         y=labels, folds=10, metric='val_acc', task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy package smlproj_deploy_2 have been saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<talos.commands.deploy.Deploy at 0x7fe5d062e590>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talos.Deploy(scan_object=scan_object, model_name='smlproj_deploy_2', metric='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "smlproj_model = talos.Restore('smlproj_deploy_2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>destination_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>3865</td>\n",
       "      <td>3924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>3917</td>\n",
       "      <td>4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3922</td>\n",
       "      <td>3947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3955</td>\n",
       "      <td>3987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3993</td>\n",
       "      <td>4063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_node  destination_node\n",
       "0               0              2917\n",
       "1               0              2956\n",
       "2               1              4038\n",
       "3               2              1848\n",
       "4               3               513\n",
       "...           ...               ...\n",
       "1995         3865              3924\n",
       "1996         3917              4025\n",
       "1997         3922              3947\n",
       "1998         3955              3987\n",
       "1999         3993              4063\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCols=['Id', 'source_node', 'destination_node'] \n",
    "df_test_public = pd.read_csv('test-public.csv')\n",
    "ids = df_test_public['Id'].values\n",
    "df_test_public.columns = testCols\n",
    "df_test_public = df_test_public.drop('Id', axis = 1)\n",
    "df_test_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_n2v = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(df_test_public['source_node'], df_test_public['destination_node'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_n2v_1 = np.array(x_testing_n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edges = df_test_public.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 2000\n"
     ]
    }
   ],
   "source": [
    "features_test = generate_features(test_edges, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test_to_csv(features):\n",
    "    with open(\"test.csv\",\"w\",newline=\"\") as csvfile:\n",
    "        writer=csv.writer(csvfile)\n",
    "        writer.writerow([\"JC\",\"AA\",\"Label\"])\n",
    "        writer.writerows(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_test_to_csv(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_sim = pd.read_csv('test.csv')\n",
    "FEATURE_SIZE=2\n",
    "\n",
    "X_test_sim = dataset_test_sim.iloc[:,:FEATURE_SIZE].values\n",
    "y_test_sim = dataset_test_sim.iloc[:,FEATURE_SIZE].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_sim = sc.transform(X_test_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats_testing = np.concatenate((x_testing_sim,x_testing_n2v_1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = smlproj_model.model.predict(all_feats_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ANN_results.csv\",\"w\",newline=\"\") as csvfile:\n",
    "    writer=csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Predicted\"])\n",
    "    test_id=1\n",
    "    for prediction in y:\n",
    "        writer.writerow([test_id,prediction[0]])\n",
    "        test_id+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
